{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Oxford-IIIT Pet Dataset\n",
    "- https://www.robots.ox.ac.uk/~vgg/data/pets/\n",
    "- 37개 카테고리의 개, 고양이 품종 데이터셋. 각 클래스 별로 대략 200여장의 이미지를 제공한다.\n",
    "- 파일명에 품종이 포함되어 있어 classification 데이터셋을 만들 수 있다.\n",
    "- 3686개 이미지에대한 annotation 파일을 제공한다. 나머지는 classification만 할 수 있음. 파일명이 lable이라서\n",
    "    - bounding box는 각 pet의 얼굴을 가리킨다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# library import\n",
    "import os\n",
    " \n",
    "import re\n",
    "import random\n",
    "import xml.etree.ElementTree as et\n",
    "from PIL import Image\n",
    " \n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    " \n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    " \n",
    "print(tf.__version__)\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oxford Pet Dataset\n",
    "- 파일명\n",
    "    - 품종명_번호.jpg\n",
    "    - 대문자로 시작: 고양이, 소문자로 시작: 개\n",
    "    - Egyptian_Mau_1.jpg, american_bulldog_10.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Downloading gdown-3.13.0.tar.gz (9.3 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "    Preparing wheel metadata: started\n",
      "    Preparing wheel metadata: finished with status 'done'\n",
      "Requirement already satisfied: requests[socks]>=2.12.0 in c:\\users\\swl\\anaconda3\\envs\\tf2\\lib\\site-packages (from gdown) (2.24.0)\n",
      "Requirement already satisfied: six in c:\\users\\swl\\anaconda3\\envs\\tf2\\lib\\site-packages (from gdown) (1.15.0)\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.60.0-py2.py3-none-any.whl (75 kB)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\swl\\anaconda3\\envs\\tf2\\lib\\site-packages (from requests[socks]>=2.12.0->gdown) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\swl\\anaconda3\\envs\\tf2\\lib\\site-packages (from requests[socks]>=2.12.0->gdown) (2020.6.20)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\swl\\anaconda3\\envs\\tf2\\lib\\site-packages (from requests[socks]>=2.12.0->gdown) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\swl\\anaconda3\\envs\\tf2\\lib\\site-packages (from requests[socks]>=2.12.0->gdown) (1.25.11)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\swl\\anaconda3\\envs\\tf2\\lib\\site-packages (from requests[socks]>=2.12.0->gdown) (1.7.1)\n",
      "Building wheels for collected packages: gdown\n",
      "  Building wheel for gdown (PEP 517): started\n",
      "  Building wheel for gdown (PEP 517): finished with status 'done'\n",
      "  Created wheel for gdown: filename=gdown-3.13.0-py3-none-any.whl size=9034 sha256=f5906a6a9ec0612593f4eedebd3a8ef8dca197950de54dd162570bcd48715aef\n",
      "  Stored in directory: c:\\users\\swl\\appdata\\local\\pip\\cache\\wheels\\2f\\2a\\2f\\86449b6bdbaa9aef873f68332b68be6bfbc386b9219f47157d\n",
      "Successfully built gdown\n",
      "Installing collected packages: tqdm, filelock, gdown\n",
      "Successfully installed filelock-3.0.12 gdown-3.13.0 tqdm-4.60.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T05:04:01.864802Z",
     "start_time": "2021-05-07T05:02:44.223626Z"
    }
   },
   "outputs": [],
   "source": [
    "import gdown\n",
    "url = 'https://drive.google.com/uc?id=1gXqmWrxJqdp_luNKZmv81vY5cjFLuTdT'\n",
    "fname = 'oxford_pet.zip'\n",
    "gdown.download(url, fname, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r oxford_pet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 압축풀기\n",
    "!unzip -q oxford_pet.zip -d oxford_pet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 압축이 풀린 directory 확인\n",
    "!ls oxford_pet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 경로 설정\n",
    "base_dir = '/content/oxford_pet' #데이터셋 기본경로\n",
    "image_dir = os.path.join(base_dir, 'images') # image 디렉토리 경로\n",
    "bbox_dir = os.path.join(base_dir, 'annotations', 'xmls') #annotation 파일경로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 파일명 조회\n",
    "image_files = [fname for fname in os.listdir(image_dir) if os.path.splitext(fname)[-1] == '.jpg']\n",
    "print(len(image_files))\n",
    " \n",
    "image_files[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#annotation 파일 경로\n",
    "bbox_files = [fname for fname in os.listdir(bbox_dir) if os.path.splitext(fname)[-1] == '.xml']\n",
    "print(len(bbox_files))\n",
    "bbox_files[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 파일중 RGB가 아닌 이미지 파일과 그 파일에 대한 annotation파일 제거\n",
    "remove_image_cnt = 0\n",
    "for image_file in image_files:\n",
    "    image_path = os.path.join(image_dir, image_file)\n",
    "    bbox_file = os.path.splitext(image_file)[0]+'.xml'\n",
    "    bbox_path = os.path.join(bbox_dir, bbox_file)\n",
    "    \n",
    "    image = Image.open(image_path)\n",
    "    image_mode = image.mode\n",
    "    if image_mode != 'RGB':\n",
    "        image = np.asarray(image)\n",
    "        print(image_file, image_mode, image.shape)\n",
    "\n",
    "        os.remove(image_path)\n",
    "        remove_image_cnt += 1\n",
    "        try:\n",
    "            os.remove(bbox_path)\n",
    "            print(bbox_path)\n",
    "        except FileNotFoundError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 삭제후 image, annotation 파일 목록 다시만들기\n",
    "image_files = [fname for fname in os.listdir(image_dir) if os.path.splitext(fname)[-1] == '.jpg']\n",
    "bbox_files = [fname for fname in os.listdir(bbox_dir) if os.path.splitext(fname)[-1] == '.xml']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class dictionary 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 클래스 딕셔너리 리스트로 만들기\n",
    "class_list = set() \n",
    "for image_file in image_files:\n",
    "    file_name = os.path.splitext(image_file)[0] \n",
    "    class_name = re.sub('_\\d+', '', file_name)\n",
    "    class_list.add(class_name)    \n",
    "class_list = list(class_list) \n",
    "class_list.sort()  \n",
    "print(len(class_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리스트 딕셔너리로 만들기: class->index로 반환하는 것.\n",
    "class2idx = {cls:idx for idx, cls in enumerate(class_list)}\n",
    "class2idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train/validation 데이터셋 만들기\n",
    "\n",
    "## TFRecord 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "N_BBOX = len(bbox_files)\n",
    "N_TRAIN = 3000 \n",
    "N_VAL = N_BBOX - N_TRAIN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TFRecord 저장할 directory 생성\n",
    "tfr_dir = os.path.join(base_dir, 'tfrecord')\n",
    "os.makedirs(tfr_dir, exist_ok=True)\n",
    "\n",
    "tfr_train_dir = os.path.join(tfr_dir, 'oxford_train.tfr')\n",
    "tfr_val_dir = os.path.join(tfr_dir, 'oxford_val.tfr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TFRecord writer 생성\n",
    "writer_train = tf.io.TFRecordWriter(tfr_train_dir)\n",
    "writer_val = tf.io.TFRecordWriter(tfr_val_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following functions can be used to convert a value to a type compatible with tf.Example.\n",
    " \n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = value.numpy() # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    " \n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    " \n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, validation 데이터셋의 index 설정\n",
    "\n",
    "# 데이터 섞기\n",
    "shuffle_list = list(range(N_BBOX))\n",
    "random.shuffle(shuffle_list) \n",
    "# 분할\n",
    "train_idx_list = shuffle_list[:N_TRAIN]\n",
    "val_idx_list = shuffle_list[N_TRAIN:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train TFRecord 생성\n",
    "for idx in train_idx_list:\n",
    "    bbox_file = bbox_files[idx]\n",
    "    bbox_path = os.path.join(bbox_dir, bbox_file)\n",
    " \n",
    "    tree = et.parse(bbox_path)\n",
    "    width = float(tree.find('./size/width').text)\n",
    "    height = float(tree.find('./size/height').text)\n",
    "    xmin = float(tree.find('./object/bndbox/xmin').text)\n",
    "    ymin = float(tree.find('./object/bndbox/ymin').text)\n",
    "    xmax = float(tree.find('./object/bndbox/xmax').text)\n",
    "    ymax = float(tree.find('./object/bndbox/ymax').text)\n",
    "    #X, Y Center 좌표\n",
    "    xc = (xmin + xmax) / 2.\n",
    "    yc = (ymin + ymax) / 2.\n",
    " \n",
    "    x = xc / width\n",
    "    y = yc / height\n",
    "\n",
    "    w = (xmax - xmin) / width\n",
    "    h = (ymax - ymin) / height\n",
    " \n",
    "    file_name = os.path.splitext(bbox_file)[0]\n",
    "    image_file = file_name + '.jpg'\n",
    "    image_path = os.path.join(image_dir, image_file)\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize((IMG_SIZE, IMG_SIZE))\n",
    "    bimage = image.tobytes()\n",
    " \n",
    "    class_name = re.sub('_\\d+', '', file_name)\n",
    "    class_num = class2idx[class_name]\n",
    "    \n",
    "    \n",
    "    if file_name[0].islower():\n",
    "        bi_cls_num = 0\n",
    "    else:\n",
    "        bi_cls_num = 1\n",
    "    \n",
    "    \n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "          'image': _bytes_feature(bimage),\n",
    "          'cls_num': _int64_feature(class_num),\n",
    "          'bi_cls_num': _int64_feature(bi_cls_num),\n",
    "          'x': _float_feature(x),\n",
    "          'y': _float_feature(y),\n",
    "          'w': _float_feature(w),\n",
    "          'h': _float_feature(h)\n",
    "    }))\n",
    "    writer_train.write(example.SerializeToString())\n",
    "\n",
    "writer_train.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation TFRecord 생성\n",
    "for idx in val_idx_list:\n",
    "    bbox_file = bbox_files[idx]\n",
    "    bbox_path = os.path.join(bbox_dir, bbox_file)\n",
    " \n",
    "    tree = et.parse(bbox_path)\n",
    "    width = float(tree.find('./size/width').text)\n",
    "    height = float(tree.find('.size/height').text)\n",
    "    xmin = float(tree.find('./object/bndbox/xmin').text)\n",
    "    ymin = float(tree.find('./object/bndbox/ymin').text)\n",
    "    xmax = float(tree.find('./object/bndbox/xmax').text)\n",
    "    ymax = float(tree.find('./object/bndbox/ymax').text)\n",
    "    xc = (xmin + xmax) / 2.\n",
    "    yc = (ymin + ymax) / 2.\n",
    "    x = xc / width\n",
    "    y = yc / height\n",
    "    w = (xmax - xmin) / width\n",
    "    h = (ymax - ymin) / height\n",
    " \n",
    "    file_name = os.path.splitext(bbox_file)[0]\n",
    "    image_file = file_name + '.jpg'\n",
    "    image_path = os.path.join(image_dir, image_file)\n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize((IMG_SIZE, IMG_SIZE))\n",
    "    bimage = image.tobytes()\n",
    " \n",
    "    class_name = re.sub('_\\d+', '', file_name)\n",
    "    class_num = class2idx[class_name]\n",
    " \n",
    "    if file_name[0].islower():\n",
    "        bi_cls_num = 0\n",
    "    else:\n",
    "        bi_cls_num = 1\n",
    "    \n",
    "    \n",
    "    example = tf.train.Example(features=tf.train.Features(feature={\n",
    "      'image': _bytes_feature(bimage),\n",
    "      'cls_num': _int64_feature(class_num),\n",
    "      'bi_cls_num': _int64_feature(bi_cls_num),\n",
    "      'x': _float_feature(x),\n",
    "      'y': _float_feature(y),\n",
    "      'w': _float_feature(w),\n",
    "      'h': _float_feature(h)\n",
    "    }))\n",
    "    writer_val.write(example.SerializeToString())\n",
    "\n",
    "writer_val.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Localization 모델 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyper Parameters\n",
    "LEARNING_RATE = 0.0001\n",
    "N_CLASS = len(class_list)\n",
    "N_EPOCHS = 40\n",
    "N_BATCH = 40\n",
    "IMG_SIZE = 224\n",
    "\n",
    "steps_per_epoch = N_TRAIN // N_BATCH\n",
    "validation_steps = int(np.ceil(N_VAL / N_BATCH)) \n",
    "\n",
    "print(steps_per_epoch, validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFRecord에 저장된 Dataset의 하나의 Data를 parsing하는 함수\n",
    "def _parse_function(tfrecord_serialized):\n",
    "    \"\"\"\n",
    "    [매개변수]\n",
    "        tfrecord_serialized: parsing할 1개의 data\n",
    "    [반환값] \n",
    "        튜플 (image, ground truth)\n",
    "    \"\"\"\n",
    "    \n",
    "    features={'image': tf.io.FixedLenFeature([], tf.string),\n",
    "              'cls_num': tf.io.FixedLenFeature([], tf.int64),\n",
    "              'bi_cls_num': tf.io.FixedLenFeature([], tf.int64),\n",
    "              'x': tf.io.FixedLenFeature([], tf.float32),\n",
    "              'y': tf.io.FixedLenFeature([], tf.float32),\n",
    "              'w': tf.io.FixedLenFeature([], tf.float32),\n",
    "              'h': tf.io.FixedLenFeature([], tf.float32)              \n",
    "             }\n",
    "    \n",
    "    parsed_features = tf.io.parse_single_example(tfrecord_serialized, features)\n",
    "    \n",
    "    \n",
    "    image = tf.io.decode_raw(parsed_features['image'], tf.uint8)    \n",
    "    image = tf.reshape(image, [IMG_SIZE, IMG_SIZE, 3])\n",
    "    image = tf.cast(image, tf.float32)/255.\n",
    "    \n",
    "    cls_label = tf.cast(parsed_features['cls_num'], tf.int64)\n",
    "    bi_cls_label = tf.cast(parsed_features['bi_cls_num'], tf.int64)\n",
    "    \n",
    "    x = tf.cast(parsed_features['x'], tf.float32)\n",
    "    y = tf.cast(parsed_features['y'], tf.float32)\n",
    "    w = tf.cast(parsed_features['w'], tf.float32)\n",
    "    h = tf.cast(parsed_features['h'], tf.float32)\n",
    "    grount_truth = tf.stack([x, y, w, h], -1)\n",
    "    \n",
    "    return image, grount_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## train dataset 만들기\n",
    "train_dataset = tf.data.TFRecordDataset(tfr_train_dir)\n",
    "train_dataset = train_dataset.map(_parse_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=N_TRAIN).prefetch(tf.data.experimental.AUTOTUNE).batch(N_BATCH).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## validation dataset 만들기\n",
    "val_dataset = tf.data.TFRecordDataset(tfr_val_dir)\n",
    "val_dataset = val_dataset.map(_parse_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(N_BATCH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trainset의 데이터 읽어서 bounding box 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-07T08:32:19.544443Z",
     "start_time": "2021-05-07T08:32:19.467249Z"
    }
   },
   "outputs": [],
   "source": [
    "for image, gt in val_dataset.take(3):\n",
    "    \n",
    "    '''그림을 그리기 위해서 bbox의 왼쪽 위 꼭지점 좌표를 계산하고, \n",
    "    xmin, ymin, w, h 각각을 image size에 맞게 scaling'''\n",
    "    x = gt[:,0] \n",
    "    y = gt[:,1] \n",
    "    w = gt[:,2] \n",
    "    h = gt[:,3] \n",
    "    xmin = x[0].numpy() - w[0].numpy()/2.\n",
    "    ymin = y[0].numpy() - h[0].numpy()/2.\n",
    "    rect_x = int(xmin * IMG_SIZE) \n",
    "    rect_y = int(ymin * IMG_SIZE)\n",
    "    rect_w = int(w[0].numpy() * IMG_SIZE)\n",
    "    rect_h = int(h[0].numpy() * IMG_SIZE)\n",
    "    \n",
    "    \n",
    "    rect = Rectangle((rect_x, rect_y), rect_w, rect_h, fill=False, color='red')\n",
    "    plt.axes().add_patch(rect)\n",
    "    plt.imshow(image[0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 생성 및 학습\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.applications import ResNet101V2\n",
    "from tensorflow.keras.layers import Conv2D, ReLU, MaxPooling2D, Dense, BatchNormalization, GlobalAveragePooling2D, Concatenate\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_l_model():\n",
    "    resnet101v2 = ResNet101V2(include_top=False, weights='imagenet', input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(resnet101v2)\n",
    "    model.add(GlobalAveragePooling2D())\n",
    "    model.add(Dense(256))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ReLU())\n",
    "    model.add(Dense(64))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(ReLU())\n",
    "    model.add(Dense(4, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_l_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 컴파일\n",
    "## learning rate scheduing\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=LEARNING_RATE,\n",
    "                                                          decay_steps=steps_per_epoch*10,\n",
    "                                                          decay_rate=0.5,\n",
    "                                                          staircase=True)\n",
    "model.compile(optimizers.Adam(lr_schedule), loss=\"mse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = r'/content/drive/MyDrive/save_models/oxford_pet_localization_resnet101v2_model'\n",
    "mc_callback = keras.callbacks.ModelCheckpoint(filepath, 'val_loss',verbose=1, save_best_only=True)\n",
    "es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, verbose=1)\n",
    "\n",
    "history = model.fit(train_dataset, steps_per_epoch=steps_per_epoch,\n",
    "                    epochs=N_EPOCHS,\n",
    "                    validation_data=val_dataset,\n",
    "                    validation_steps=validation_steps, \n",
    "                    callbacks=[mc_callback, es_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 미리학습한 모델 다운로드\n",
    "import gdown\n",
    "url = 'https://drive.google.com/uc?id=1-2IbiHp3SdffxkqIj4iGL9-recS6g697'\n",
    "fname = 'oxford_pet_localization_resnet101.tar.gz'\n",
    "gdown.download(url, fname, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tar -zxvf oxford_pet_localization_resnet101.tar.gz -C  models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 모델 load\n",
    "filepath = '/content/models/oxford_pet_localization_resnet101v2_model'\n",
    "saved_model = keras.models.load_model(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bounding Box 그리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측한 bounding box와 ground truth box를 image에 같이 표시\n",
    "# 정답은 빨간색 box, 예측은 파란색 box\n",
    "idx = 0\n",
    "num_imgs = validation_steps\n",
    "for val_data, val_gt in val_dataset.take(num_imgs):\n",
    "\n",
    "    x = val_gt[:,0]\n",
    "    y = val_gt[:,1]\n",
    "    w = val_gt[:,2]\n",
    "    h = val_gt[:,3]\n",
    "\n",
    "    xmin = x[idx].numpy() - w[idx].numpy()/2.\n",
    "    ymin = y[idx].numpy() - h[idx].numpy()/2.\n",
    "\n",
    "    rect_x = int(xmin * IMG_SIZE)\n",
    "    rect_y = int(ymin * IMG_SIZE)\n",
    "    rect_w = int(w[idx].numpy() * IMG_SIZE)\n",
    "    rect_h = int(h[idx].numpy() * IMG_SIZE)\n",
    "    \n",
    "\n",
    "    rect = Rectangle((rect_x, rect_y), rect_w, rect_h, fill=False, color='red')\n",
    "    plt.axes().add_patch(rect)\n",
    "    \n",
    "    prediction = saved_model.predict(val_data)\n",
    "    pred_x = prediction[:,0]\n",
    "    pred_y = prediction[:,1]\n",
    "    pred_w = prediction[:,2]\n",
    "    pred_h = prediction[:,3]\n",
    "    pred_xmin = pred_x[idx] - pred_w[idx]/2.\n",
    "    pred_ymin = pred_y[idx] - pred_h[idx]/2.\n",
    "    pred_rect_x = int(pred_xmin * IMG_SIZE)\n",
    "    pred_rect_y = int(pred_ymin * IMG_SIZE)\n",
    "    pred_rect_w = int(pred_w[idx] * IMG_SIZE)\n",
    "    pred_rect_h = int(pred_h[idx] * IMG_SIZE)\n",
    "    \n",
    "    pred_rect = Rectangle((pred_rect_x, pred_rect_y), pred_rect_w, pred_rect_h,\n",
    "                         fill=False, color='blue')\n",
    "    plt.axes().add_patch(pred_rect)\n",
    "    \n",
    "    \n",
    "    plt.imshow(val_data[idx])\n",
    "    plt.show()\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IoU확인하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Valiation set의 IOU 계산\n",
    "avg_iou = 0\n",
    "num_imgs = validation_steps\n",
    "res = N_VAL % N_BATCH\n",
    "for i, (val_data, val_gt) in enumerate(val_dataset.take(num_imgs)):\n",
    "    \n",
    "    flag = (i == validation_steps-1)\n",
    "    x = val_gt[:,0]\n",
    "    y = val_gt[:,1]\n",
    "    w = val_gt[:,2]\n",
    "    h = val_gt[:,3]\n",
    "    prediction = saved_model.predict(val_data)\n",
    "    pred_x = prediction[:,0]\n",
    "    pred_y = prediction[:,1]\n",
    "    pred_w = prediction[:,2]\n",
    "    pred_h = prediction[:,3]\n",
    "    for idx in range(N_BATCH):\n",
    "        if(flag):\n",
    "            if idx == res:\n",
    "                flag = False\n",
    "                break  \n",
    "        \n",
    "        xmin = int((x[idx].numpy() - w[idx].numpy()/2.)*IMG_SIZE)\n",
    "        ymin = int((y[idx].numpy() - h[idx].numpy()/2.)*IMG_SIZE)\n",
    "        xmax = int((x[idx].numpy() + w[idx].numpy()/2.)*IMG_SIZE)\n",
    "        ymax = int((y[idx].numpy() + h[idx].numpy()/2.)*IMG_SIZE)\n",
    "        \n",
    "        pred_xmin = int((pred_x[idx] - pred_w[idx]/2.)*IMG_SIZE)\n",
    "        pred_ymin = int((pred_y[idx] - pred_h[idx]/2.)*IMG_SIZE)\n",
    "        pred_xmax = int((pred_x[idx] + pred_w[idx]/2.)*IMG_SIZE)\n",
    "        pred_ymax = int((pred_y[idx] + pred_h[idx]/2.)*IMG_SIZE)\n",
    " \n",
    "        if xmin > pred_xmax or xmax < pred_xmin:        \n",
    "            continue\n",
    "        if ymin > pred_ymax or ymax < pred_ymin:        \n",
    "            continue\n",
    "        \n",
    "        gt_width = xmax-xmin\n",
    "        gt_height = ymax - ymin\n",
    "        pred_width = pred_xmax - pred_xmin\n",
    "        pred_height = pred_ymax - pred_ymin\n",
    " \n",
    "        inter_width  =  np.min((xmax, pred_xmax)) - np.max((xmin, pred_xmin))\n",
    "        inter_height = np.min((ymax, pred_ymax)) - np.max((ymin, pred_ymin))\n",
    "  \n",
    " \n",
    "        iou = (inter_width * inter_height)/((gt_width * gt_height) + (pred_width * pred_height) - (inter_width * inter_height))\n",
    "\n",
    "        avg_iou += iou / N_VAL\n",
    "\n",
    "print(avg_iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification을 추가하여 Multi-task Learning으로 Localization 학습하기\n",
    "\n",
    "- **고양이/개 2개 class로 classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.applications import ResNet101V2\n",
    "from tensorflow.keras.layers import Conv2D, ReLU, MaxPooling2D, Dense, BatchNormalization, GlobalAveragePooling2D, Concatenate\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tfrecord 파싱 함수(classification + localization)\n",
    "def _parse_function(tfrecord_serialized):\n",
    "    features={'image': tf.io.FixedLenFeature([], tf.string),\n",
    "              'cls_num': tf.io.FixedLenFeature([], tf.int64),\n",
    "              'bi_cls_num': tf.io.FixedLenFeature([], tf.int64),\n",
    "              'x': tf.io.FixedLenFeature([], tf.float32),\n",
    "              'y': tf.io.FixedLenFeature([], tf.float32),\n",
    "              'w': tf.io.FixedLenFeature([], tf.float32),\n",
    "              'h': tf.io.FixedLenFeature([], tf.float32)              \n",
    "             }\n",
    "    parsed_features = tf.io.parse_single_example(tfrecord_serialized, features)\n",
    "    \n",
    "    image = tf.io.decode_raw(parsed_features['image'], tf.uint8)    \n",
    "    image = tf.reshape(image, [IMG_SIZE, IMG_SIZE, 3])\n",
    "    image = tf.cast(image, tf.float32)/255.\n",
    "\n",
    "    cls_label = tf.cast(parsed_features['cls_num'], tf.float32)\n",
    "    bi_cls_label = tf.cast(parsed_features['bi_cls_num'], tf.float32)\n",
    "    \n",
    "    x = tf.cast(parsed_features['x'], tf.float32)\n",
    "    y = tf.cast(parsed_features['y'], tf.float32)\n",
    "    w = tf.cast(parsed_features['w'], tf.float32)\n",
    "    h = tf.cast(parsed_features['h'], tf.float32)\n",
    "    ground_truth = tf.stack([bi_cls_label, x, y, w, h], -1) \n",
    "    \n",
    "    return image, ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Dataset 생성\n",
    "train_dataset = tf.data.TFRecordDataset(tfr_train_dir)\n",
    "train_dataset = train_dataset.map(_parse_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train_dataset = train_dataset.shuffle(buffer_size=N_TRAIN).prefetch(\n",
    "    tf.data.experimental.AUTOTUNE).batch(N_BATCH).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validation Dataset 생성\n",
    "val_dataset = tf.data.TFRecordDataset(tfr_val_dir)\n",
    "val_dataset = val_dataset.map(_parse_function, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "val_dataset = val_dataset.batch(N_BATCH).repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cl_model():    \n",
    "    resnet101v2 = ResNet101V2(include_top=False, weights='imagenet', input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "    gap = GlobalAveragePooling2D()(resnet101v2.output)\n",
    "\n",
    "    dense_b1_1 = Dense(256)(gap)\n",
    "    bn_b1_2 = BatchNormalization()(dense_b1_1)\n",
    "    relu_b1_3 = ReLU()(bn_b1_2)\n",
    "    dense_b1_4 = Dense(64)(relu_b1_3)\n",
    "    bn_b1_5 = BatchNormalization()(dense_b1_4)\n",
    "    relu_b1_6 = ReLU()(bn_b1_5)\n",
    "    output1 = Dense(2, activation='softmax', name='output1')(relu_b1_6)\n",
    "\n",
    "    dense_b2_1 = Dense(256)(gap)\n",
    "    bn_b2_2 = BatchNormalization()(dense_b2_1)\n",
    "    relu_b2_3 = ReLU()(bn_b2_2)\n",
    "    dense_b2_4 = Dense(64)(relu_b2_3)\n",
    "    bn_b2_5 = BatchNormalization()(dense_b2_4)\n",
    "    relu_b2_6 = ReLU()(bn_b2_5)\n",
    "    output2 = Dense(4, activation='sigmoid', name='output2')(relu_b2_6)\n",
    "    \n",
    "    concat = Concatenate(name='finaly_output')([output1, output2])\n",
    "    return keras.Model(inputs=resnet101v2.input, outputs=concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_cl_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss 함수 구현\n",
    "def loss_fn(y_true, y_pred):\n",
    "    cls_labels = tf.cast(y_true[:,:1], tf.int64)\n",
    "    loc_labels = y_true[:,1:]\n",
    "    cls_preds = y_pred[:,:2]\n",
    "    loc_preds = y_pred[:,2:]\n",
    "    cls_loss = tf.keras.losses.SparseCategoricalCrossentropy()(cls_labels, cls_preds)\n",
    "    loc_loss = tf.keras.losses.MeanSquaredError()(loc_labels, loc_preds)\n",
    "    return cls_loss + 5*loc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 컴파일\n",
    "## learning rate scheduing\n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=LEARNING_RATE,\n",
    "                                                          decay_steps=steps_per_epoch*10,\n",
    "                                                          decay_rate=0.5,\n",
    "                                                          staircase=True)\n",
    "model.compile(optimizers.Adam(lr_schedule), loss=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습\n",
    "filepath2 = r'/content/drive/MyDrive/save_models/oxford_pet_localization_classification_resnet101v2_weights/oxford_pet_lc_weights.ckpt'\n",
    "mc_callback = keras.callbacks.ModelCheckpoint(filepath2, 'val_loss',verbose=1, save_best_only=True, save_weights_only=True)\n",
    "es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n",
    "\n",
    "history = model.fit(train_dataset, steps_per_epoch=steps_per_epoch,\n",
    "                    epochs=N_EPOCHS,\n",
    "                    validation_data=val_dataset,\n",
    "                    validation_steps=validation_steps,\n",
    "                    callbacks=[mc_callback, es_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 미리 학습된 weights 가져오기 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 미리학습한 모델 다운로드\n",
    "import gdown\n",
    "url = 'https://drive.google.com/uc?id=1ycRNri9Gr6QjcOFv4GQi_DLCU17JbOyo'\n",
    "fname = 'oxford_pet_classification_localization_resnet101_weight.tar.gz'\n",
    "gdown.download(url, fname, quiet=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 압축풀기\n",
    "!tar -zxvf oxford_pet_classification_localization_resnet101_weight.tar.gz -C  models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 마지막으로 저장된 checkpoint 경로 확인\n",
    "best_weight_path = tf.train.latest_checkpoint('/content/models/oxford_pet_localization_classification_resnet101v2_weights')\n",
    "best_weight_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장된 weight load \n",
    "saved_model2 = create_cl_model()\n",
    "saved_model2.load_weights(best_weight_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bounding Box 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배치중 idx번째 것만 확인\n",
    "idx = 1\n",
    "num_imgs = validation_steps \n",
    "for val_data, val_gt in val_dataset.take(num_imgs):\n",
    "    \n",
    "   \n",
    "    gt_cls_name = np.where(val_gt[:,0]==0,'dog','cat')\n",
    "    \n",
    "    x = val_gt[:,1]\n",
    "    y = val_gt[:,2]\n",
    "    w = val_gt[:,3]\n",
    "    h = val_gt[:,4]\n",
    "    xmin = x[idx].numpy() - w[idx].numpy()/2.\n",
    "    ymin = y[idx].numpy() - h[idx].numpy()/2.\n",
    "    rect_x = int(xmin * IMG_SIZE)\n",
    "    rect_y = int(ymin * IMG_SIZE)\n",
    "    rect_w = int(w[idx].numpy() * IMG_SIZE)\n",
    "    rect_h = int(h[idx].numpy() * IMG_SIZE)\n",
    "    \n",
    "    rect = Rectangle((rect_x, rect_y), rect_w, rect_h, fill=False, color='red')\n",
    "    plt.axes().add_patch(rect)    \n",
    "    \n",
    "    prediction = saved_model2.predict(val_data)\n",
    "    \n",
    "    pred_cls_idx = np.argmax(prediction[:,:2], axis=-1)\n",
    "    pred_cls_name = np.where(pred_cls_idx==0, 'dog','cat')\n",
    "    \n",
    "    pred_x = prediction[:,2]\n",
    "    pred_y = prediction[:,3]\n",
    "    pred_w = prediction[:,4]\n",
    "    pred_h = prediction[:,5]\n",
    "    pred_xmin = pred_x[idx] - pred_w[idx]/2.\n",
    "    pred_ymin = pred_y[idx] - pred_h[idx]/2.\n",
    "    pred_rect_x = int(pred_xmin * IMG_SIZE)\n",
    "    pred_rect_y = int(pred_ymin * IMG_SIZE)\n",
    "    pred_rect_w = int(pred_w[idx] * IMG_SIZE)\n",
    "    pred_rect_h = int(pred_h[idx] * IMG_SIZE)\n",
    "    \n",
    "    pred_rect = Rectangle((pred_rect_x, pred_rect_y), pred_rect_w, pred_rect_h,\n",
    "                         fill=False, color='blue')\n",
    "    plt.axes().add_patch(pred_rect)\n",
    "    plt.title(f'Ground Truth-{gt_cls_name[idx]}, Pred:{pred_cls_name[idx]}')\n",
    "    \n",
    "    plt.imshow(val_data[idx])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IoU 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_iou = 0\n",
    "num_imgs = validation_steps\n",
    "res = N_VAL % N_BATCH\n",
    "for i, (val_data, val_gt) in enumerate(val_dataset.take(num_imgs)):    \n",
    "    flag = (i == validation_steps-1)\n",
    "    x = val_gt[:,1]\n",
    "    y = val_gt[:,2]\n",
    "    w = val_gt[:,3]\n",
    "    h = val_gt[:,4]\n",
    "\n",
    "    prediction = saved_model2.predict(val_data)\n",
    "    \n",
    "    pred_x = prediction[:,2]\n",
    "    pred_y = prediction[:,3]\n",
    "    pred_w = prediction[:,4]\n",
    "    pred_h = prediction[:,5]\n",
    "    for idx in range(N_BATCH):\n",
    "        if(flag):\n",
    "            if idx == res:\n",
    "                flag = False\n",
    "                break          \n",
    "        xmin = int((x[idx].numpy() - w[idx].numpy()/2.)*IMG_SIZE)\n",
    "        ymin = int((y[idx].numpy() - h[idx].numpy()/2.)*IMG_SIZE)\n",
    "        xmax = int((x[idx].numpy() + w[idx].numpy()/2.)*IMG_SIZE)\n",
    "        ymax = int((y[idx].numpy() + h[idx].numpy()/2.)*IMG_SIZE)\n",
    "        \n",
    "        pred_xmin = int((pred_x[idx] - pred_w[idx]/2.)*IMG_SIZE)\n",
    "        pred_ymin = int((pred_y[idx] - pred_h[idx]/2.)*IMG_SIZE)\n",
    "        pred_xmax = int((pred_x[idx] + pred_w[idx]/2.)*IMG_SIZE)\n",
    "        pred_ymax = int((pred_y[idx] + pred_h[idx]/2.)*IMG_SIZE)\n",
    " \n",
    "        if xmin > pred_xmax or xmax < pred_xmin:        \n",
    "            continue\n",
    "        if ymin > pred_ymax or ymax < pred_ymin:        \n",
    "            continue\n",
    "        \n",
    "        gt_width = xmax-xmin\n",
    "        gt_height = ymax - ymin\n",
    "        pred_width = pred_xmax - pred_xmin\n",
    "        pred_height = pred_ymax - pred_ymin\n",
    " \n",
    "        inter_width  =  np.min((xmax, pred_xmax)) - np.max((xmin, pred_xmin))\n",
    "        inter_height = np.min((ymax, pred_ymax)) - np.max((ymin, pred_ymin))\n",
    "  \n",
    " \n",
    "        iou = (inter_width * inter_height)/((gt_width * gt_height) + (pred_width * pred_height) - (inter_width * inter_height))\n",
    "        avg_iou += iou / N_VAL\n",
    "        \n",
    "print(avg_iou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 새로운 Image로 Test하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "image = Image.open('dog.jpg')\n",
    "image = image.resize((224, 224))\n",
    "image = np.array(image)\n",
    "image = image/255.\n",
    "image = image[np.newaxis, ...]\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 결과 확인 - bounding box, class\n",
    "from matplotlib.patches import Rectangle\n",
    "prediction = saved_model2.predict(image)\n",
    "pred_cls = np.where(np.argmax(prediction[0,:2], axis=-1)==0, 'dog', 'cat')\n",
    "print(pred_cls)\n",
    "pred_x = prediction[0,2]\n",
    "pred_y = prediction[0,3]\n",
    "pred_w = prediction[0,4]\n",
    "pred_h = prediction[0,5]\n",
    "pred_xmin = pred_x - pred_w/2.\n",
    "pred_ymin = pred_y - pred_h/2.\n",
    "pred_rect_x = int(pred_xmin * IMG_SIZE)\n",
    "pred_rect_y = int(pred_ymin * IMG_SIZE)\n",
    "pred_rect_w = int(pred_w * IMG_SIZE)\n",
    "pred_rect_h = int(pred_h * IMG_SIZE)\n",
    "\n",
    "pred_rect = Rectangle((pred_rect_x, pred_rect_y), pred_rect_w, pred_rect_h,\n",
    "                       fill=False, color='red')\n",
    "plt.axes().add_patch(pred_rect)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(image[0])\n",
    "plt.title(f'Prediction class:{pred_cls}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
